{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright (c) Microsoft Corporation. All rights reserved.\n",
    "\n",
    "Licensed under the MIT License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Impressions](https://PixelServer20190423114238.azurewebsites.net/api/impressions/NotebookVM/how-to-use-azureml/machine-learning-pipelines/pipeline-style-transfer/pipeline-style-transfer.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural style transfer on video\n",
    "Using modified code from `pytorch`'s neural style [example](https://pytorch.org/tutorials/advanced/neural_style_tutorial.html), we show how to setup a pipeline for doing style transfer on video. The pipeline has following steps:\n",
    "1. Split a video into images\n",
    "2. Run neural style on each image using one of the provided models (from `pytorch` pretrained models for this example).\n",
    "3. Stitch the image back into a video."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "If you are using an Azure Machine Learning Notebook VM, you are all set. Otherwise, make sure you go through the configuration Notebook located at https://github.com/Azure/MachineLearningNotebooks first if you haven't. This sets you up with a working config file that has information on your workspace, subscription id, etc. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Workspace\n",
    "\n",
    "Initialize a workspace object from persisted configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Workspace name: azureml-demos-wks\n",
      "Azure region: westeurope\n",
      "Subscription id: 81ae6a7a-0699-4b60-9c61-294bae201fba\n",
      "Resource group: azureml-demos-rg\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from azureml.core import Workspace, Experiment\n",
    "\n",
    "ws = Workspace.from_config()\n",
    "print('Workspace name: ' + ws.name, \n",
    "      'Azure region: ' + ws.location, \n",
    "      'Subscription id: ' + ws.subscription_id, \n",
    "      'Resource group: ' + ws.resource_group, sep = '\\n')\n",
    "\n",
    "scripts_folder = \"scripts_folder\"\n",
    "\n",
    "if not os.path.isdir(scripts_folder):\n",
    "    os.mkdir(scripts_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.compute import AmlCompute, ComputeTarget\n",
    "from azureml.core.datastore import Datastore\n",
    "from azureml.data.data_reference import DataReference\n",
    "from azureml.pipeline.core import Pipeline, PipelineData\n",
    "from azureml.pipeline.steps import PythonScriptStep, MpiStep\n",
    "from azureml.core.runconfig import CondaDependencies, RunConfiguration\n",
    "from azureml.core.compute_target import ComputeTargetException"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create or use existing compute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found existing cluster.\n",
      "found existing cluster.\n"
     ]
    }
   ],
   "source": [
    "# AmlCompute\n",
    "cpu_cluster_name = \"cpu-cluster\"\n",
    "try:\n",
    "    cpu_cluster = AmlCompute(ws, cpu_cluster_name)\n",
    "    print(\"found existing cluster.\")\n",
    "except ComputeTargetException:\n",
    "    print(\"creating new cluster\")\n",
    "    provisioning_config = AmlCompute.provisioning_configuration(vm_size = \"STANDARD_D2_v2\",\n",
    "                                                                    max_nodes = 1)\n",
    "\n",
    "    # create the cluster\n",
    "    cpu_cluster = ComputeTarget.create(ws, cpu_cluster_name, provisioning_config)\n",
    "    cpu_cluster.wait_for_completion(show_output=True)\n",
    "    \n",
    "# AmlCompute\n",
    "gpu_cluster_name = \"gpu-cluster\"\n",
    "try:\n",
    "    gpu_cluster = AmlCompute(ws, gpu_cluster_name)\n",
    "    print(\"found existing cluster.\")\n",
    "except ComputeTargetException:\n",
    "    print(\"creating new cluster\")\n",
    "    provisioning_config = AmlCompute.provisioning_configuration(vm_size = \"STANDARD_NC6\",\n",
    "                                                                    max_nodes = 3)\n",
    "\n",
    "    # create the cluster\n",
    "    gpu_cluster = ComputeTarget.create(ws, gpu_cluster_name, provisioning_config)\n",
    "    gpu_cluster.wait_for_completion(show_output=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Python Scripts\n",
    "We use an edited version of `neural_style_mpi.py` (original is [here](https://github.com/pytorch/examples/blob/master/fast_neural_style/neural_style/neural_style.py)). Scripts to split and stitch the video are thin wrappers to calls to `ffmpeg`. \n",
    "\n",
    "We install `ffmpeg` through conda dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'scripts_folder/neural_style_mpi.py'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import shutil\n",
    "shutil.copy(\"neural_style_mpi.py\", scripts_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting scripts_folder/process_video.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile $scripts_folder/process_video.py\n",
    "import argparse\n",
    "import glob\n",
    "import os\n",
    "import subprocess\n",
    "\n",
    "parser = argparse.ArgumentParser(description=\"Process input video\")\n",
    "parser.add_argument('--input_video', required=True)\n",
    "parser.add_argument('--output_audio', required=True)\n",
    "parser.add_argument('--output_images', required=True)\n",
    "\n",
    "args = parser.parse_args()\n",
    "\n",
    "os.makedirs(args.output_audio, exist_ok=True)\n",
    "os.makedirs(args.output_images, exist_ok=True)\n",
    "\n",
    "subprocess.run(\"ffmpeg -i {} {}/video.aac\"\n",
    "              .format(args.input_video, args.output_audio),\n",
    "               shell=True, check=True\n",
    "              )\n",
    "\n",
    "subprocess.run(\"ffmpeg -i {} {}/%05d_video.jpg -hide_banner\"\n",
    "              .format(args.input_video, args.output_images),\n",
    "               shell=True, check=True\n",
    "              )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting scripts_folder/stitch_video.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile $scripts_folder/stitch_video.py\n",
    "import argparse\n",
    "import os\n",
    "import subprocess\n",
    "\n",
    "parser = argparse.ArgumentParser(description=\"Process input video\")\n",
    "parser.add_argument('--images_dir', required=True)\n",
    "parser.add_argument('--input_audio', required=True)\n",
    "parser.add_argument('--output_dir', required=True)\n",
    "\n",
    "args = parser.parse_args()\n",
    "\n",
    "os.makedirs(args.output_dir, exist_ok=True)\n",
    "\n",
    "subprocess.run(\"ffmpeg -framerate 30 -i {}/%05d_video.jpg -c:v libx264 -profile:v high -crf 20 -pix_fmt yuv420p \"\n",
    "               \"-y {}/video_without_audio.mp4\"\n",
    "               .format(args.images_dir, args.output_dir),\n",
    "               shell=True, check=True\n",
    "              )\n",
    "\n",
    "subprocess.run(\"ffmpeg -i {}/video_without_audio.mp4 -i {}/video.aac -map 0:0 -map 1:0 -vcodec \"\n",
    "               \"copy -acodec copy -y {}/video_with_audio.mp4\"\n",
    "               .format(args.output_dir, args.input_audio, args.output_dir),\n",
    "               shell=True, check=True\n",
    "              )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sample video **organutan.mp4** is stored at a publicly shared datastore. We are registering the datastore below. If you want to take a look at the original video, click here. (https://pipelinedata.blob.core.windows.net/sample-videos/orangutan.mp4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting youtube-dl\n",
      "  Downloading https://files.pythonhosted.org/packages/42/9c/9e13d8c2cb43dc158ede19e5dade9037fa5ee321e70494a3960d62f9242b/youtube_dl-2019.9.12.1-py2.py3-none-any.whl (1.8MB)\n",
      "\u001b[K    100% |████████████████████████████████| 1.8MB 705kB/s eta 0:00:01\n",
      "\u001b[?25hInstalling collected packages: youtube-dl\n",
      "Successfully installed youtube-dl-2019.9.12.1\n",
      "\u001b[33mYou are using pip version 9.0.1, however version 19.2.3 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install youtube-dl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[youtube] RQJ__qk5UD8: Downloading webpage\n",
      "[youtube] RQJ__qk5UD8: Downloading video info webpage\n",
      "[download] Destination: Discover Kosice - Slovakia edition-RQJ__qk5UD8.mp4\n",
      "\u001b[K[download] 100% of 16.59MiB in 00:0315MiB/s ETA 00:002\n"
     ]
    }
   ],
   "source": [
    "!youtube-dl https://www.youtube.com/watch?v=RQJ__qk5UD8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mv \"Discover Kosice - Slovakia edition-RQJ__qk5UD8.mp4\" discover_kosice.mp4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 17M\n",
      "drwxrwxrwx 2 root root    0 Sep 23 18:30 .\n",
      "drwxrwxrwx 2 root root    0 Sep 23 18:30 ..\n",
      "-rwxrwxrwx 1 root root  17M Feb 18  2019 discover_kosice.mp4\n",
      "drwxrwxrwx 2 root root    0 Sep 23 18:48 .ipynb_checkpoints\n",
      "-rwxrwxrwx 1 root root 8.0K Sep 23 18:30 neural_style_mpi.py\n",
      "-rwxrwxrwx 1 root root 7.1K Sep 23 18:30 neural_style.py\n",
      "-rwxrwxrwx 1 root root  27K Sep 23 19:00 pipeline-style-transfer.ipynb\n",
      "-rwxrwxrwx 1 root root  106 Sep 23 18:30 pipeline-style-transfer.yml\n",
      "drwxrwxrwx 2 root root    0 Sep 23 18:49 scripts_folder\n"
     ]
    }
   ],
   "source": [
    "!ls -lah"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<video width=\"640\" height=\"480\" controls>\n",
       "  <source src=\"./discover_kosice.mp4\" type=\"video/mp4\">\n",
       "</video>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%HTML\n",
    "<video width=\"640\" height=\"480\" controls>\n",
    "  <source src=\"./discover_kosice.mp4\" type=\"video/mp4\">\n",
    "</video>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'azuremldstoragef8faad157__azureml_blobstore_8978bdbe_dc0a_4ba7_9461_d89c82c30f5a': <azureml.data.azure_storage_datastore.AzureBlobDatastore at 0x7f37045559e8>,\n",
       " 'fgnet': <azureml.data.azure_storage_datastore.AzureBlobDatastore at 0x7f3704550d68>,\n",
       " 'images_datastore': <azureml.data.azure_storage_datastore.AzureBlobDatastore at 0x7f3704550780>,\n",
       " 'models': <azureml.data.azure_storage_datastore.AzureBlobDatastore at 0x7f37045619b0>,\n",
       " 'workspaceblobstore': <azureml.data.azure_storage_datastore.AzureBlobDatastore at 0x7f37045662e8>,\n",
       " 'workspacefilestore': <azureml.data.azure_storage_datastore.AzureFileDatastore at 0x7f370793d080>}"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ws.datastores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# the default blob store attached to a workspace\n",
    "default_datastore = ws.get_default_datastore()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/azmnt/code/Users/anvykhod/samples-1.0.62/how-to-use-azureml/machine-learning-pipelines/pipeline-style-transfer\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploading an estimated of 1 files\n",
      "Uploading ./discover_kosice.mp4\n",
      "Uploaded ./discover_kosice.mp4, 1 files out of an estimated total of 1\n",
      "Uploaded 1 files\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "$AZUREML_DATAREFERENCE_929aaa3df0154e0fa27276ef9cd5c05d"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "default_datastore.upload_files(files=[\"./discover_kosice.mp4\"], target_path = \"input_videos\", overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# datastore for input video\n",
    "account_name = \"pipelinedata\"\n",
    "#video_ds = Datastore.register_azure_blob_container(ws, \"videos\", \"sample-videos\",\n",
    "#                                            account_name=account_name, overwrite=True)\n",
    "\n",
    "# datastore for models\n",
    "models_ds = Datastore.register_azure_blob_container(ws, \"models\", \"styletransfer\", \n",
    "                                                        account_name=\"pipelinedata\", \n",
    "                                                        overwrite=True)\n",
    "                                                        \n",
    "# downloaded models from https://pytorch.org/tutorials/advanced/neural_style_tutorial.html are kept here\n",
    "models_dir = DataReference(data_reference_name=\"models\", datastore=models_ds, \n",
    "                           path_on_datastore=\"saved_models\", mode=\"download\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sample video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "?DataReference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "VIDEO_NAME = \"discover_kosice.mp4\"\n",
    "kosice_video = DataReference(datastore=default_datastore, path_on_datastore=\"input_videos/\" + VIDEO_NAME,\n",
    "                            data_reference_name=\"video\",mode=\"download\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING - 'gpu_support' is no longer necessary; AzureML now automatically detects and uses nvidia docker extension when it is available. It will be removed in a future release.\n",
      "WARNING - 'gpu_support' is no longer necessary; AzureML now automatically detects and uses nvidia docker extension when it is available. It will be removed in a future release.\n"
     ]
    }
   ],
   "source": [
    "cd = CondaDependencies()\n",
    "\n",
    "cd.add_channel(\"conda-forge\")\n",
    "cd.add_conda_package(\"ffmpeg\")\n",
    "\n",
    "cd.add_channel(\"pytorch\")\n",
    "cd.add_conda_package(\"pytorch\")\n",
    "cd.add_conda_package(\"torchvision\")\n",
    "\n",
    "# Runconfig\n",
    "amlcompute_run_config = RunConfiguration(conda_dependencies=cd)\n",
    "amlcompute_run_config.environment.docker.enabled = True\n",
    "amlcompute_run_config.environment.docker.gpu_support = True\n",
    "amlcompute_run_config.environment.docker.base_image = \"pytorch/pytorch\"\n",
    "amlcompute_run_config.environment.spark.precache_packages = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "ffmpeg_audio = PipelineData(name=\"ffmpeg_audio\", datastore=default_datastore)\n",
    "ffmpeg_images = PipelineData(name=\"ffmpeg_images\", datastore=default_datastore)\n",
    "processed_images = PipelineData(name=\"processed_images\", datastore=default_datastore)\n",
    "output_video = PipelineData(name=\"output_video\", datastore=default_datastore)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define tweakable parameters to pipeline\n",
    "These parameters can be changed when the pipeline is published and rerun from a REST call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.pipeline.core.graph import PipelineParameter\n",
    "# create a parameter for style (one of \"candy\", \"mosaic\", \"rain_princess\", \"udnie\") to transfer the images to\n",
    "style_param = PipelineParameter(name=\"style\", default_value=\"mosaic\")\n",
    "# create a parameter for the number of nodes to use in step no. 2 (style transfer)\n",
    "nodecount_param = PipelineParameter(name=\"nodecount\", default_value=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING - 'distributed_backend' parameter will be deprecated. Please use 'distributed_training' instead.\n",
      "WARNING - 'gpu_support' is no longer necessary; AzureML now automatically detects and uses nvidia docker extension when it is available. It will be removed in a future release.\n",
      "WARNING - 'gpu_support' is no longer necessary; AzureML now automatically detects and uses nvidia docker extension when it is available. It will be removed in a future release.\n"
     ]
    }
   ],
   "source": [
    "split_video_step = PythonScriptStep(\n",
    "    name=\"split video\",\n",
    "    script_name=\"process_video.py\",\n",
    "    arguments=[\"--input_video\", kosice_video,\n",
    "               \"--output_audio\", ffmpeg_audio,\n",
    "               \"--output_images\", ffmpeg_images,\n",
    "              ],\n",
    "    compute_target=cpu_cluster,\n",
    "    inputs=[kosice_video],\n",
    "    outputs=[ffmpeg_images, ffmpeg_audio],\n",
    "    runconfig=amlcompute_run_config,\n",
    "    source_directory=scripts_folder\n",
    ")\n",
    "\n",
    "# create a MPI step for distributing style transfer step across multiple nodes in AmlCompute \n",
    "# using 'nodecount_param' PipelineParameter\n",
    "distributed_style_transfer_step = MpiStep(\n",
    "    name=\"mpi style transfer\",\n",
    "    script_name=\"neural_style_mpi.py\",\n",
    "    arguments=[\"--content-dir\", ffmpeg_images,\n",
    "               \"--output-dir\", processed_images,\n",
    "               \"--model-dir\", models_dir,\n",
    "               \"--style\", style_param,\n",
    "               \"--cuda\", 1\n",
    "              ],\n",
    "    compute_target=gpu_cluster,\n",
    "    node_count=nodecount_param, \n",
    "    process_count_per_node=1,\n",
    "    inputs=[models_dir, ffmpeg_images],\n",
    "    outputs=[processed_images],\n",
    "    pip_packages=[\"mpi4py\", \"torch\", \"torchvision\"],\n",
    "    use_gpu=True,\n",
    "    source_directory=scripts_folder\n",
    ")\n",
    "\n",
    "stitch_video_step = PythonScriptStep(\n",
    "    name=\"stitch\",\n",
    "    script_name=\"stitch_video.py\",\n",
    "    arguments=[\"--images_dir\", processed_images, \n",
    "               \"--input_audio\", ffmpeg_audio, \n",
    "               \"--output_dir\", output_video],\n",
    "    compute_target=cpu_cluster,\n",
    "    inputs=[processed_images, ffmpeg_audio],\n",
    "    outputs=[output_video],\n",
    "    runconfig=amlcompute_run_config,\n",
    "    source_directory=scripts_folder\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING - 'gpu_support' is no longer necessary; AzureML now automatically detects and uses nvidia docker extension when it is available. It will be removed in a future release.\n",
      "WARNING - 'gpu_support' is no longer necessary; AzureML now automatically detects and uses nvidia docker extension when it is available. It will be removed in a future release.\n",
      "WARNING - 'gpu_support' is no longer necessary; AzureML now automatically detects and uses nvidia docker extension when it is available. It will be removed in a future release.\n"
     ]
    }
   ],
   "source": [
    "pipeline = Pipeline(workspace=ws, steps=[stitch_video_step])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created step stitch [0c1d6d86][bc81f3b4-bab1-40c9-8e5f-1a45339f60c8], (This step is eligible to reuse a previous run's output)\n",
      "Created step mpi style transfer [db20c4d5][0bfeca03-d9e2-4161-92a7-72bf6d4ad07b], (This step is eligible to reuse a previous run's output)\n",
      "Created step split video [c3da7443][dc2abb78-74fd-4d4a-908b-c4fab8591fe9], (This step is eligible to reuse a previous run's output)\n",
      "Using data reference models for StepId [3ad0cf76][b84084c5-ac21-4834-82d8-0185476faa29], (Consumers of this data are eligible to reuse prior runs.)\n",
      "Using data reference video for StepId [104f71d6][6784f084-1129-4a2f-9d52-e578cd404d6f], (Consumers of this data are eligible to reuse prior runs.)\n",
      "Submitted pipeline run: d44c2c87-a761-4a45-a531-0ad93202054b\n"
     ]
    }
   ],
   "source": [
    "# submit the pipeline and provide values for the PipelineParameters used in the pipeline\n",
    "pipeline_run = Experiment(ws, 'style_transfer').submit(pipeline, pipeline_parameters={\"style\": \"mosaic\", \"nodecount\": 3})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Monitor using widget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.pipeline.core.run import PipelineRun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.widgets import RunDetails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b3a96cec5564328aa7ad8963adaae39",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "feb1ceed59ae47869c681859e12d7e25",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "RunDetails(pipeline_run).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp = Experiment(ws, 'style_transfer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table style=\"width:100%\"><tr><th>Experiment</th><th>Id</th><th>Type</th><th>Status</th><th>Details Page</th><th>Docs Page</th></tr><tr><td>style_transfer</td><td>75ad2944-6ecc-46da-b9a1-5b80767b9cdb</td><td>azureml.PipelineRun</td><td>Completed</td><td><a href=\"https://mlworkspace.azure.ai/portal/subscriptions/81ae6a7a-0699-4b60-9c61-294bae201fba/resourceGroups/azureml-demos-rg/providers/Microsoft.MachineLearningServices/workspaces/azureml-demos-wks/experiments/style_transfer/runs/75ad2944-6ecc-46da-b9a1-5b80767b9cdb\" target=\"_blank\" rel=\"noopener\">Link to Azure Portal</a></td><td><a href=\"https://docs.microsoft.com/en-us/python/api/overview/azure/ml/intro?view=azure-ml-py\" target=\"_blank\" rel=\"noopener\">Link to Documentation</a></td></tr></table>"
      ],
      "text/plain": [
       "Run(Experiment: style_transfer,\n",
       "Id: 75ad2944-6ecc-46da-b9a1-5b80767b9cdb,\n",
       "Type: azureml.PipelineRun,\n",
       "Status: Completed)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline_run = PipelineRun(exp, \"75ad2944-6ecc-46da-b9a1-5b80767b9cdb\")\n",
    "\n",
    "pipeline_run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "95789797549d4c2bae6c6027b66254d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "RunDetails(pipeline_run).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Downloads the video in `output_video` folder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download output video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_video(run, target_dir=None):\n",
    "    stitch_run = run.find_step_run(\"stitch\")[0]\n",
    "    port_data = stitch_run.get_output_data(\"output_video\")\n",
    "    port_data.download(target_dir, show_progress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PipelineRunId: 75ad2944-6ecc-46da-b9a1-5b80767b9cdb\n",
      "Link to Portal: https://mlworkspace.azure.ai/portal/subscriptions/81ae6a7a-0699-4b60-9c61-294bae201fba/resourceGroups/azureml-demos-rg/providers/Microsoft.MachineLearningServices/workspaces/azureml-demos-wks/experiments/style_transfer/runs/75ad2944-6ecc-46da-b9a1-5b80767b9cdb\n",
      "\n",
      "PipelineRun Execution Summary\n",
      "==============================\n",
      "PipelineRun Status: Finished\n",
      "{'runId': '75ad2944-6ecc-46da-b9a1-5b80767b9cdb', 'status': 'Completed', 'startTimeUtc': '2019-09-23T19:21:17.368753Z', 'endTimeUtc': '2019-09-23T20:13:38.68828Z', 'properties': {'azureml.runsource': 'azureml.PipelineRun', 'runSource': None, 'runType': 'HTTP', 'azureml.parameters': '{\"style\":\"mosaic\",\"nodecount\":\"3\"}'}, 'logFiles': {'logs/azureml/stdoutlogs.txt': 'https://azuremldstoragef8faad157.blob.core.windows.net/azureml/ExperimentRun/dcid.75ad2944-6ecc-46da-b9a1-5b80767b9cdb/logs/azureml/stdoutlogs.txt?sv=2018-11-09&sr=b&sig=J6A6zEovLtfyoe4F4ChAwN%2F4XIF8wcqCyYFDFsBksdI%3D&st=2019-09-23T20%3A05%3A04Z&se=2019-09-24T04%3A15%3A04Z&sp=r', 'logs/azureml/stderrlogs.txt': 'https://azuremldstoragef8faad157.blob.core.windows.net/azureml/ExperimentRun/dcid.75ad2944-6ecc-46da-b9a1-5b80767b9cdb/logs/azureml/stderrlogs.txt?sv=2018-11-09&sr=b&sig=f%2BgWfCnQV0agt3%2BMuqlFH%2F1sf7D1c2u0yCYarleTztw%3D&st=2019-09-23T20%3A05%3A04Z&se=2019-09-24T04%3A15%3A04Z&sp=r', 'logs/azureml/executionlogs.txt': 'https://azuremldstoragef8faad157.blob.core.windows.net/azureml/ExperimentRun/dcid.75ad2944-6ecc-46da-b9a1-5b80767b9cdb/logs/azureml/executionlogs.txt?sv=2018-11-09&sr=b&sig=g8EER13bISpTLHnC9uEv4JJ42oUZgDTGIIwvLB%2Fuag8%3D&st=2019-09-23T20%3A05%3A04Z&se=2019-09-24T04%3A15%3A04Z&sp=r'}}\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Finished'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline_run.wait_for_completion()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading azureml/b41e8a80-10e0-4163-89c6-853213aacfb9/output_video/video_with_audio.mp4\n",
      "Downloading azureml/b41e8a80-10e0-4163-89c6-853213aacfb9/output_video/video_without_audio.mp4\n",
      "Downloaded azureml/b41e8a80-10e0-4163-89c6-853213aacfb9/output_video/video_with_audio.mp4, 1 files out of an estimated total of 2\n",
      "Downloaded azureml/b41e8a80-10e0-4163-89c6-853213aacfb9/output_video/video_without_audio.mp4, 2 files out of an estimated total of 2\n"
     ]
    }
   ],
   "source": [
    "download_video(pipeline_run, \"output_video_mosaic\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "video_with_audio.mp4  video_without_audio.mp4\r\n"
     ]
    }
   ],
   "source": [
    "!ls output_video_mosaic/azureml/b41e8a80-10e0-4163-89c6-853213aacfb9/output_video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./discover_kosice_styled.mp4'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import shutil\n",
    "shutil.move(\"output_video_mosaic/azureml/%s/output_video/video_with_audio.mp4\" % pipeline_run.find_step_run(\"stitch\")[0].id, \"./discover_kosice_styled.mp4\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "discover_kosice.mp4\t    output_video_mosaic\r\n",
      "discover_kosice_styled.mp4  pipeline-style-transfer.ipynb\r\n",
      "neural_style_mpi.py\t    pipeline-style-transfer.yml\r\n",
      "neural_style.py\t\t    scripts_folder\r\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<video width=\"640\" height=\"480\" controls>\n",
       "  <source src=\"./discover_kosice_styled_mosaic.mp4\" type=\"video/mp4\">\n",
       "</video>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%HTML\n",
    "<video width=\"640\" height=\"480\" controls>\n",
    "  <source src=\"./discover_kosice_styled_mosaic.mp4\" type=\"video/mp4\">\n",
    "</video>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Publish pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table style=\"width:100%\"><tr><th>Name</th><th>Id</th><th>Status</th><th>Endpoint</th></tr><tr><td>kosice batch score style transfer</td><td><a href=\"https://mlworkspace.azure.ai/portal/subscriptions/81ae6a7a-0699-4b60-9c61-294bae201fba/resourceGroups/azureml-demos-rg/providers/Microsoft.MachineLearningServices/workspaces/azureml-demos-wks/pipelines/09dffea6-12bc-4c24-988b-914605fb4e05\" target=\"_blank\" rel=\"noopener\">09dffea6-12bc-4c24-988b-914605fb4e05</a></td><td>Active</td><td><a href=\"https://westeurope.aether.ms/api/v1.0/subscriptions/81ae6a7a-0699-4b60-9c61-294bae201fba/resourceGroups/azureml-demos-rg/providers/Microsoft.MachineLearningServices/workspaces/azureml-demos-wks/PipelineRuns/PipelineSubmit/09dffea6-12bc-4c24-988b-914605fb4e05\" target=\"_blank\" rel=\"noopener\">REST Endpoint</a></td></tr></table>"
      ],
      "text/plain": [
       "Pipeline(Name: kosice batch score style transfer,\n",
       "Id: 09dffea6-12bc-4c24-988b-914605fb4e05,\n",
       "Status: Active,\n",
       "Endpoint: https://westeurope.aether.ms/api/v1.0/subscriptions/81ae6a7a-0699-4b60-9c61-294bae201fba/resourceGroups/azureml-demos-rg/providers/Microsoft.MachineLearningServices/workspaces/azureml-demos-wks/PipelineRuns/PipelineSubmit/09dffea6-12bc-4c24-988b-914605fb4e05)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "published_pipeline = pipeline_run.publish_pipeline(\n",
    "    name=\"kosice batch score style transfer\", description=\"style transfer\", version=\"1.0\")\n",
    "\n",
    "published_pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get published pipeline\n",
    "\n",
    "You can get the published pipeline using **pipeline id**.\n",
    "\n",
    "To get all the published pipelines for a given workspace(ws): \n",
    "```css\n",
    "all_pub_pipelines = PublishedPipeline.get_all(ws)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.pipeline.core import PublishedPipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_id = published_pipeline.id # use your published pipeline id\n",
    "published_pipeline = PublishedPipeline.get(ws, pipeline_id)\n",
    "\n",
    "published_pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Re-run pipeline through REST calls for other styles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get AAD token\n",
    "[This notebook](https://aka.ms/pl-restep-auth) shows how to authenticate to AML workspace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.authentication import InteractiveLoginAuthentication\n",
    "import requests\n",
    "\n",
    "auth = InteractiveLoginAuthentication()\n",
    "aad_token = auth.get_authentication_header()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING - PublishedPipeline.get_all(workspace) is being deprecated. Use PublishedPipeline.list(workspace) instead.\n"
     ]
    }
   ],
   "source": [
    "published_pipeline = PublishedPipeline.get_all(ws)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table style=\"width:100%\"><tr><th>Name</th><th>Id</th><th>Status</th><th>Endpoint</th></tr><tr><td>kosice batch score style transfer</td><td><a href=\"https://mlworkspace.azure.ai/portal/subscriptions/81ae6a7a-0699-4b60-9c61-294bae201fba/resourceGroups/azureml-demos-rg/providers/Microsoft.MachineLearningServices/workspaces/azureml-demos-wks/pipelines/09dffea6-12bc-4c24-988b-914605fb4e05\" target=\"_blank\" rel=\"noopener\">09dffea6-12bc-4c24-988b-914605fb4e05</a></td><td>Active</td><td><a href=\"https://westeurope.aether.ms/api/v1.0/subscriptions/81ae6a7a-0699-4b60-9c61-294bae201fba/resourceGroups/azureml-demos-rg/providers/Microsoft.MachineLearningServices/workspaces/azureml-demos-wks/PipelineRuns/PipelineSubmit/09dffea6-12bc-4c24-988b-914605fb4e05\" target=\"_blank\" rel=\"noopener\">REST Endpoint</a></td></tr></table>"
      ],
      "text/plain": [
       "Pipeline(Name: kosice batch score style transfer,\n",
       "Id: 09dffea6-12bc-4c24-988b-914605fb4e05,\n",
       "Status: Active,\n",
       "Endpoint: https://westeurope.aether.ms/api/v1.0/subscriptions/81ae6a7a-0699-4b60-9c61-294bae201fba/resourceGroups/azureml-demos-rg/providers/Microsoft.MachineLearningServices/workspaces/azureml-demos-wks/PipelineRuns/PipelineSubmit/09dffea6-12bc-4c24-988b-914605fb4e05)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "published_pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get endpoint URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "rest_endpoint = published_pipeline.endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://westeurope.aether.ms/api/v1.0/subscriptions/81ae6a7a-0699-4b60-9c61-294bae201fba/resourceGroups/azureml-demos-rg/providers/Microsoft.MachineLearningServices/workspaces/azureml-demos-wks/PipelineRuns/PipelineSubmit/09dffea6-12bc-4c24-988b-914605fb4e05'"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rest_endpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Send request and monitor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run the pipeline using PipelineParameter values style='candy' and nodecount=2\n",
    "response = requests.post(rest_endpoint, \n",
    "                         headers=aad_token,\n",
    "                         json={\"ExperimentName\": \"style_transfer\",\n",
    "                               \"ParameterAssignments\": {\"style\": \"candy\", \"nodecount\": 2}})                         \n",
    "run_id = response.json()[\"Id\"]\n",
    "\n",
    "from azureml.pipeline.core.run import PipelineRun\n",
    "published_pipeline_run_candy = PipelineRun(ws.experiments[\"style_transfer\"], run_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp = Experiment(ws, 'style_transfer')\n",
    "published_pipeline_run_candy = [r for r in exp.get_runs() if \"candy\" in r.get_properties()[\"azureml.parameters\"]][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table style=\"width:100%\"><tr><th>Experiment</th><th>Id</th><th>Type</th><th>Status</th><th>Details Page</th><th>Docs Page</th></tr><tr><td>style_transfer</td><td>237d6fc1-e282-4ab0-84e0-f3581bdf9d24</td><td>azureml.PipelineRun</td><td>Completed</td><td><a href=\"https://mlworkspace.azure.ai/portal/subscriptions/81ae6a7a-0699-4b60-9c61-294bae201fba/resourceGroups/azureml-demos-rg/providers/Microsoft.MachineLearningServices/workspaces/azureml-demos-wks/experiments/style_transfer/runs/237d6fc1-e282-4ab0-84e0-f3581bdf9d24\" target=\"_blank\" rel=\"noopener\">Link to Azure Portal</a></td><td><a href=\"https://docs.microsoft.com/en-us/python/api/overview/azure/ml/intro?view=azure-ml-py\" target=\"_blank\" rel=\"noopener\">Link to Documentation</a></td></tr></table>"
      ],
      "text/plain": [
       "Run(Experiment: style_transfer,\n",
       "Id: 237d6fc1-e282-4ab0-84e0-f3581bdf9d24,\n",
       "Type: azureml.PipelineRun,\n",
       "Status: Completed)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "published_pipeline_run_candy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52c13badcc944d16b7193279ade7261a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "RunDetails(published_pipeline_run_candy).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_video(run, target_dir=None, style = None):\n",
    "    stitch_run = run.find_step_run(\"stitch\")[0]\n",
    "    port_data = stitch_run.get_output_data(\"output_video\")\n",
    "    port_data.download(target_dir, show_progress=True)\n",
    "    fname = \"./discover_kosice_styled\" + ((\"_\" + style) if style else \"\") + \".mp4\"\n",
    "    print(fname)\n",
    "    shutil.move(\"./azureml/%s/output_video/video_with_audio.mp4\" % stitch_run.id, fname)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING - Path already exists. Skipping download for ./azureml/927fb143-f480-45cd-a5af-6c4fc33410f9/output_video/video_with_audio.mp4\n",
      "WARNING - Path already exists. Skipping download for ./azureml/927fb143-f480-45cd-a5af-6c4fc33410f9/output_video/video_without_audio.mp4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./discover_kosice_styled_candy.mp4\n"
     ]
    }
   ],
   "source": [
    "download_video(published_pipeline_run_candy, \".\", style = \"candy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "azureml\t\t\t\t   neural_style.py\r\n",
      "discover_kosice.mp4\t\t   output_video_mosaic\r\n",
      "discover_kosice_styled_candy.mp4   pipeline-style-transfer.ipynb\r\n",
      "discover_kosice_styled_mosaic.mp4  pipeline-style-transfer.yml\r\n",
      "neural_style_mpi.py\t\t   scripts_folder\r\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<video width=\"640\" height=\"480\" controls>\n",
       "  <source src=\"./discover_kosice_styled_candy.mp4\" type=\"video/mp4\">\n",
       "</video>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%HTML\n",
    "<video width=\"640\" height=\"480\" controls>\n",
    "  <source src=\"./discover_kosice_styled_candy.mp4\" type=\"video/mp4\">\n",
    "</video>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run the pipeline using PipelineParameter values style='rain_princess' and nodecount=3\n",
    "response = requests.post(rest_endpoint, \n",
    "                         headers=aad_token,\n",
    "                         json={\"ExperimentName\": \"style_transfer\",\n",
    "                               \"ParameterAssignments\": {\"style\": \"rain_princess\", \"nodecount\": 3}})    \n",
    "run_id = response.json()[\"Id\"]\n",
    "\n",
    "published_pipeline_run_rain = PipelineRun(ws.experiments[\"style_transfer\"], run_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b5413b3eada942c0b9dead1d7025a958",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "RunDetails(published_pipeline_run_rain).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run the pipeline using PipelineParameter values style='udnie' and nodecount=4\n",
    "response = requests.post(rest_endpoint, \n",
    "                         headers=aad_token,\n",
    "                         json={\"ExperimentName\": \"style_transfer\",\n",
    "                               \"ParameterAssignments\": {\"style\": \"udnie\", \"nodecount\": 3}})   \n",
    "run_id = response.json()[\"Id\"]\n",
    "\n",
    "published_pipeline_run_udnie = PipelineRun(ws.experiments[\"style_transfer\"], run_id)\n",
    "\n",
    "RunDetails(published_pipeline_run_udnie).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download output from re-run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "published_pipeline_run_candy.wait_for_completion()\n",
    "published_pipeline_run_rain.wait_for_completion()\n",
    "published_pipeline_run_udnie.wait_for_completion()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "download_video(published_pipeline_run_candy, target_dir=\"output_video_candy\")\n",
    "download_video(published_pipeline_run_rain, target_dir=\"output_video_rain_princess\")\n",
    "download_video(published_pipeline_run_udnie, target_dir=\"output_video_udnie\")"
   ]
  }
 ],
 "metadata": {
  "authors": [
   {
    "name": "sanpil"
   }
  ],
  "categories": [
   "how-to-use-azureml",
   "machine-learning-pipelines"
  ],
  "kernelspec": {
   "display_name": "Python 3.6 - AzureML",
   "language": "python",
   "name": "python3-azureml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
